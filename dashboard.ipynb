{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMokXSCeMjBO",
        "outputId": "ed9f219a-4b9a-4694-c97f-3c1b0ba4d2af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSszlez2Tx2_",
        "outputId": "ed065a74-37c0-432b-dbe1-4bc9d395fa28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.44.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.4)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.34.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul7so-j7MeK6",
        "outputId": "f68d563c-3aab-4a4e-9b10-99da086c6a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dashboard code has been saved to 'kepler_dashboard.py'\n",
            "Starting Streamlit with fixed file...\n",
            "Streamlit app URL: NgrokTunnel: \"https://4cba-34-125-71-103.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "Click the URL above to view your interactive Kepler exoplanet dashboard!\n"
          ]
        }
      ],
      "source": [
        "streamlit_code = '''\n",
        "# Set page config\n",
        "st.set_page_config(\n",
        "    page_title=\"Exoplanet Classification Model Comparison\",\n",
        "    page_icon=\"ü™ê  \",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\",\n",
        ")\n",
        "\n",
        "# Define a colorblind-friendly palette (Okabe-Ito)\n",
        "COLORBLIND_PALETTE = {\n",
        "    'blue': '#0072B2',\n",
        "    'orange': '#E69F00',\n",
        "    'green': '#009E73',\n",
        "    'red': '#D55E00',\n",
        "    'purple': '#CC79A7',\n",
        "    'yellow': '#F0E442',\n",
        "    'cyan': '#56B4E9',\n",
        "    'grey': '#999999'\n",
        "}\n",
        "\n",
        "# Define colors for the three model types\n",
        "MODEL_COLORS = {\n",
        "    'kNN': COLORBLIND_PALETTE['blue'],\n",
        "    'SVM': COLORBLIND_PALETTE['orange'],\n",
        "    'Neural Network': COLORBLIND_PALETTE['purple']\n",
        "}\n",
        "\n",
        "# Colors for the three classes (consistent across all visualizations)\n",
        "CLASS_COLORS = {\n",
        "    'FALSE POSITIVE': COLORBLIND_PALETTE['red'],\n",
        "    'CANDIDATE': COLORBLIND_PALETTE['blue'],\n",
        "    'CONFIRMED': COLORBLIND_PALETTE['green']\n",
        "}\n",
        "\n",
        "# Colors for with/without PCA comparison\n",
        "PCA_COLORS = {\n",
        "    'Without PCA': COLORBLIND_PALETTE['orange'],\n",
        "    'With PCA': COLORBLIND_PALETTE['purple']\n",
        "}\n",
        "\n",
        "# Add custom CSS to make the dashboard look better\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "    .stTabs [data-baseweb=\"tab-list\"] {\n",
        "        gap: 2px;\n",
        "    }\n",
        "    .stTabs [data-baseweb=\"tab\"] {\n",
        "        height: 50px;\n",
        "        white-space: pre-wrap;\n",
        "        background-color: #f0f2f6;\n",
        "        border-radius: 4px 4px 0px 0px;\n",
        "        gap: 1px;\n",
        "        padding-top: 10px;\n",
        "        padding-bottom: 10px;\n",
        "    }\n",
        "    .stTabs [aria-selected=\"true\"] {\n",
        "        background-color: #e6f0ff;\n",
        "    }\n",
        "    div[data-testid=\"stSidebarNav\"] li div a {\n",
        "        margin-left: 1rem;\n",
        "        padding: 1rem;\n",
        "        width: 300px;\n",
        "    }\n",
        "    div[data-testid=\"stSidebarNav\"] li div::focus-visible {\n",
        "        background-color: rgba(151, 166, 195, 0.15);\n",
        "    }\n",
        "    .metric-box {\n",
        "        background-color: #f0f2f6;\n",
        "        border-radius: 5px;\n",
        "        padding: 10px;\n",
        "        margin: 10px 0px;\n",
        "        text-align: center;\n",
        "    }\n",
        "    .metric-value {\n",
        "        font-size: 24px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "    .metric-label {\n",
        "        font-size: 14px;\n",
        "        color: #555;\n",
        "    }\n",
        "    .highlight {\n",
        "        background-color: #e6f0ff;\n",
        "        padding: 1px 4px;\n",
        "        border-radius: 3px;\n",
        "    }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Create sidebar\n",
        "st.sidebar.title(\"ü™ê   Exoplanet Classification\")\n",
        "\n",
        "# Add performance metrics for each model\n",
        "@st.cache_data\n",
        "def generate_model_metrics():\n",
        "    \"\"\"Generate performance metrics for the models\"\"\"\n",
        "    np.random.seed(42)\n",
        "    model_names = ['kNN', 'SVM', 'Neural Network']\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "    # Create a DataFrame with model performances\n",
        "    # We'll create data for models with and without PCA\n",
        "    data = []\n",
        "\n",
        "    # Define base metrics with real values matching the images provided\n",
        "    base_values = {\n",
        "        'kNN': {'accuracy': 0.8009, 'precision': 0.7915, 'recall': 0.8009, 'f1': 0.7895},\n",
        "        'SVM': {'accuracy': 0.8516, 'precision': 0.8463, 'recall': 0.8516, 'f1': 0.8394},  # Updated from Image 4\n",
        "        'Neural Network': {'accuracy': 0.82, 'precision': 0.81, 'recall': 0.80, 'f1': 0.80}\n",
        "    }\n",
        "\n",
        "    # PCA effects based on the provided image data for each model\n",
        "    pca_effects = {\n",
        "        'kNN': {'accuracy': -0.009, 'precision': -0.0122, 'recall': -0.009, 'f1': -0.0112},\n",
        "        'SVM': {'accuracy': -0.0019, 'precision': 0.0002, 'recall': -0.0019, 'f1': -0.0065},  # Updated from Image 4\n",
        "        'Neural Network': {'accuracy': -0.02, 'precision': -0.03, 'recall': -0.02, 'f1': -0.03}\n",
        "    }\n",
        "\n",
        "    # Training times (seconds)\n",
        "    training_times = {\n",
        "        'kNN': {'without_pca': 15.3, 'with_pca': 5.4},  # ~65% reduction\n",
        "        'SVM': {'without_pca': 785.5, 'with_pca': 817.2},  # 4.03% increase, from Image 4\n",
        "        'Neural Network': {'without_pca': 210.7, 'with_pca': 85.4}\n",
        "    }\n",
        "\n",
        "    # Memory usage (MB)\n",
        "    memory_usage = {\n",
        "        'kNN': {'without_pca': 55.2, 'with_pca': 2.1},  # ~96% reduction\n",
        "        'SVM': {'without_pca': 65.8, 'with_pca': 12.65},  # 80.78% reduction, from Image 4\n",
        "        'Neural Network': {'without_pca': 120.5, 'with_pca': 85.7}\n",
        "    }\n",
        "\n",
        "    # Create data for models without PCA\n",
        "    for model in model_names:\n",
        "        model_data = {\n",
        "            'model': model,\n",
        "            'pca': 'Without PCA',\n",
        "            'training_time': training_times[model]['without_pca'],\n",
        "            'memory_usage': memory_usage[model]['without_pca'],\n",
        "        }\n",
        "\n",
        "        # Use exact values from base_values\n",
        "        for metric in metrics:\n",
        "            model_data[metric] = base_values[model][metric]\n",
        "\n",
        "        data.append(model_data)\n",
        "\n",
        "    # Create data for models with PCA\n",
        "    for model in model_names:\n",
        "        model_data = {\n",
        "            'model': model,\n",
        "            'pca': 'With PCA',\n",
        "            'training_time': training_times[model]['with_pca'],\n",
        "            'memory_usage': memory_usage[model]['with_pca'],\n",
        "        }\n",
        "\n",
        "        # Add performance metrics with PCA effect\n",
        "        for metric in metrics:\n",
        "            base = base_values[model][metric]\n",
        "            pca_effect = pca_effects[model][metric]\n",
        "            model_data[metric] = base + pca_effect\n",
        "\n",
        "        data.append(model_data)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "@st.cache_data\n",
        "def generate_class_metrics():\n",
        "    \"\"\"Generate class-specific performance metrics\"\"\"\n",
        "    class_names = ['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED']\n",
        "    model_names = ['kNN', 'SVM', 'Neural Network']\n",
        "    metrics = ['precision', 'recall', 'f1']\n",
        "    pca_options = ['Without PCA', 'With PCA']\n",
        "\n",
        "    # Base values for each class and model - updated with values from image 2\n",
        "    base_values = {\n",
        "        'kNN': {\n",
        "            'FALSE POSITIVE': {'precision': 0.94, 'recall': 0.97, 'f1': 0.955},\n",
        "            'CANDIDATE': {'precision': 0.49, 'recall': 0.31, 'f1': 0.380},\n",
        "            'CONFIRMED': {'precision': 0.70, 'recall': 0.86, 'f1': 0.775}\n",
        "        },\n",
        "        'SVM': {\n",
        "            'FALSE POSITIVE': {'precision': 0.995, 'recall': 0.997, 'f1': 0.996},\n",
        "            'CANDIDATE': {'precision': 0.58, 'recall': 0.42, 'f1': 0.488},\n",
        "            'CONFIRMED': {'precision': 0.78, 'recall': 0.84, 'f1': 0.807}\n",
        "        },\n",
        "        'Neural Network': {\n",
        "            'FALSE POSITIVE': {'precision': 0.93, 'recall': 0.95, 'f1': 0.940},\n",
        "            'CANDIDATE': {'precision': 0.57, 'recall': 0.48, 'f1': 0.522},\n",
        "            'CONFIRMED': {'precision': 0.80, 'recall': 0.85, 'f1': 0.824}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # PCA effects for each class and model - based on the differences in Image 2\n",
        "    pca_effects = {\n",
        "        'kNN': {\n",
        "            'FALSE POSITIVE': {'precision': -0.01, 'recall': 0.00, 'f1': -0.002},\n",
        "            'CANDIDATE': {'precision': -0.05, 'recall': -0.02, 'f1': -0.031},\n",
        "            'CONFIRMED': {'precision': -0.02, 'recall': -0.01, 'f1': -0.014}\n",
        "        },\n",
        "        'SVM': {\n",
        "            'FALSE POSITIVE': {'precision': 0.003, 'recall': 0.0, 'f1': 0.001},\n",
        "            'CANDIDATE': {'precision': -0.05, 'recall': -0.04, 'f1': -0.04},\n",
        "            'CONFIRMED': {'precision': 0.005, 'recall': 0.0, 'f1': 0.001}\n",
        "        },\n",
        "        'Neural Network': {\n",
        "            'FALSE POSITIVE': {'precision': -0.02, 'recall': -0.01, 'f1': -0.015},\n",
        "            'CANDIDATE': {'precision': -0.04, 'recall': -0.03, 'f1': -0.035},\n",
        "            'CONFIRMED': {'precision': -0.02, 'recall': -0.01, 'f1': -0.015}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # Populate data for each model\n",
        "    for model in model_names:\n",
        "        for class_name in class_names:\n",
        "            for pca in pca_options:\n",
        "                row = {\n",
        "                    'model': model,\n",
        "                    'class': class_name,\n",
        "                    'pca': pca\n",
        "                }\n",
        "\n",
        "                for metric in metrics:\n",
        "                    base = base_values[model][class_name][metric]\n",
        "                    if pca == 'With PCA':\n",
        "                        effect = pca_effects[model][class_name][metric]\n",
        "                    else:\n",
        "                        effect = 0\n",
        "\n",
        "                    # Use exact values without randomness\n",
        "                    row[metric] = base + effect\n",
        "\n",
        "                data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "@st.cache_data\n",
        "def generate_confusion_matrices():\n",
        "    \"\"\"Generate confusion matrices for each model and PCA option\"\"\"\n",
        "    class_names = ['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED']\n",
        "    model_names = ['kNN', 'SVM', 'Neural Network']\n",
        "    pca_options = ['Without PCA', 'With PCA']\n",
        "\n",
        "    # Define base confusion matrices based on image 1 for SVM\n",
        "    base_cms = {\n",
        "        'kNN': {\n",
        "            'Without PCA': np.array([\n",
        "                [703, 30, 16],   # FALSE POSITIVE predictions\n",
        "                [16, 89, 180],   # CANDIDATE predictions\n",
        "                [4, 64, 455]     # CONFIRMED predictions\n",
        "            ]),\n",
        "            'With PCA': np.array([\n",
        "                [705, 24, 20],\n",
        "                [19, 80, 186],\n",
        "                [6, 69, 448]\n",
        "            ])\n",
        "        },\n",
        "        'SVM': {\n",
        "            'Without PCA': np.array([\n",
        "                [746, 3, 0],     # Updated from Image 1\n",
        "                [3, 110, 172],   # FALSE POSITIVE predictions\n",
        "                [0, 53, 470]     # CANDIDATE predictions\n",
        "            ]),                  # CONFIRMED predictions\n",
        "            'With PCA': np.array([\n",
        "                [746, 3, 0],     # Updated from Image 1\n",
        "                [2, 95, 188],\n",
        "                [0, 41, 482]\n",
        "            ])\n",
        "        },\n",
        "        'Neural Network': {\n",
        "            'Without PCA': np.array([\n",
        "                [730, 40, 30],\n",
        "                [70, 240, 90],\n",
        "                [40, 70, 490]\n",
        "            ]),\n",
        "            'With PCA': np.array([\n",
        "                [720, 50, 30],\n",
        "                [80, 220, 100],\n",
        "                [50, 70, 480]\n",
        "            ])\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Return as a nested dictionary for easy access\n",
        "    return base_cms\n",
        "\n",
        "# Generate neural network training history\n",
        "@st.cache_data\n",
        "def generate_nn_training_history():\n",
        "    # Actual values from the DeepLearning notebook\n",
        "    epochs = list(range(1, 31))\n",
        "    train_losses = [\n",
        "        7008.9152, 618.2007, 101.6274, 1.9714, 1.2824, 1.2649, 1.2892, 1.0943, 1.0720, 1.0396,\n",
        "        0.9747, 1.0473, 1.0142, 0.9812, 0.9838, 0.9794, 0.9897, 0.9680, 1.0010, 0.9792,\n",
        "        0.9779, 0.9778, 0.9860, 0.9854, 0.9764, 0.9814, 0.9959, 0.9684, 0.9766, 0.9857\n",
        "    ]\n",
        "    val_losses = [\n",
        "        46.1613, 6.7669, 1.1405, 0.9602, 0.9762, 0.9784, 0.9800, 0.9822, 0.9838, 0.9807,\n",
        "        0.9721, 0.9723, 0.9805, 0.9859, 0.9678, 0.9685, 0.9925, 0.9635, 0.9780, 0.9799,\n",
        "        0.9950, 0.9807, 0.9968, 0.9915, 1.0039, 0.9992, 0.9973, 0.9862, 1.0105, 1.0027\n",
        "    ]\n",
        "\n",
        "    # Final accuracies\n",
        "    final_train_acc = 0.82\n",
        "    final_val_acc = 0.78\n",
        "    final_test_acc = 0.50\n",
        "\n",
        "    return {\n",
        "        'epochs': epochs,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'final_train_acc': final_train_acc,\n",
        "        'final_val_acc': final_val_acc,\n",
        "        'final_test_acc': final_test_acc\n",
        "    }\n",
        "\n",
        "# Load PCA visualization data\n",
        "@st.cache_data\n",
        "def generate_pca_data():\n",
        "    # Create simulated PCA data for demonstration\n",
        "    np.random.seed(42)\n",
        "    n_samples = 1000\n",
        "\n",
        "    # Generate PCA components for the three classes\n",
        "    pca_data = []\n",
        "\n",
        "    # Class 0: FALSE POSITIVE\n",
        "    n_class0 = 500\n",
        "    pc1_class0 = np.random.normal(1, 1.5, n_class0)\n",
        "    pc2_class0 = np.random.normal(-1, 1, n_class0)\n",
        "    pc3_class0 = np.random.normal(0.5, 1, n_class0)\n",
        "    class0_data = pd.DataFrame({\n",
        "        'PC1': pc1_class0,\n",
        "        'PC2': pc2_class0,\n",
        "        'PC3': pc3_class0,\n",
        "        'Class': ['FALSE POSITIVE'] * n_class0\n",
        "    })\n",
        "\n",
        "    # Class 1: CANDIDATE\n",
        "    n_class1 = 200\n",
        "    pc1_class1 = np.random.normal(-1, 1, n_class1)\n",
        "    pc2_class1 = np.random.normal(1, 1.2, n_class1)\n",
        "    pc3_class1 = np.random.normal(-0.5, 1, n_class1)\n",
        "    class1_data = pd.DataFrame({\n",
        "        'PC1': pc1_class1,\n",
        "        'PC2': pc2_class1,\n",
        "        'PC3': pc3_class1,\n",
        "        'Class': ['CANDIDATE'] * n_class1\n",
        "    })\n",
        "\n",
        "    # Class 2: CONFIRMED\n",
        "    n_class2 = 300\n",
        "    pc1_class2 = np.random.normal(0, 1, n_class2)\n",
        "    pc2_class2 = np.random.normal(0, 1, n_class2)\n",
        "    pc3_class2 = np.random.normal(1.5, 1, n_class2)\n",
        "    class2_data = pd.DataFrame({\n",
        "        'PC1': pc1_class2,\n",
        "        'PC2': pc2_class2,\n",
        "        'PC3': pc3_class2,\n",
        "        'Class': ['CONFIRMED'] * n_class2\n",
        "    })\n",
        "\n",
        "    # Combine all data\n",
        "    pca_df = pd.concat([class0_data, class1_data, class2_data], ignore_index=True)\n",
        "\n",
        "    # Add explained variance\n",
        "    explained_variance = [0.45, 0.28, 0.12]  # 45%, 28%, 12% for PC1, PC2, PC3\n",
        "\n",
        "    return pca_df, explained_variance\n",
        "\n",
        "# Load models data\n",
        "model_metrics_df = generate_model_metrics()\n",
        "class_metrics_df = generate_class_metrics()\n",
        "confusion_matrices = generate_confusion_matrices()\n",
        "nn_history = generate_nn_training_history()\n",
        "pca_df, explained_variance = generate_pca_data()\n",
        "\n",
        "# Sidebar - model selection\n",
        "st.sidebar.header(\"Filter Options\")\n",
        "selected_models = st.sidebar.multiselect(\n",
        "    \"Select Models to Compare\",\n",
        "    options=['kNN', 'SVM', 'Neural Network'],\n",
        "    default=['kNN', 'SVM', 'Neural Network']\n",
        ")\n",
        "\n",
        "# PCA selection\n",
        "pca_option = st.sidebar.radio(\n",
        "    \"Dimensionality Reduction\",\n",
        "    options=['Both', 'Without PCA', 'With PCA']\n",
        ")\n",
        "\n",
        "if pca_option == 'Both':\n",
        "    pca_filter = ['Without PCA', 'With PCA']\n",
        "else:\n",
        "    pca_filter = [pca_option]\n",
        "\n",
        "# Filter data based on selections\n",
        "filtered_metrics = model_metrics_df[\n",
        "    (model_metrics_df['model'].isin(selected_models)) &\n",
        "    (model_metrics_df['pca'].isin(pca_filter))\n",
        "]\n",
        "\n",
        "filtered_class_metrics = class_metrics_df[\n",
        "    (class_metrics_df['model'].isin(selected_models)) &\n",
        "    (class_metrics_df['pca'].isin(pca_filter))\n",
        "]\n",
        "\n",
        "# Title\n",
        "st.title(\"ü™ê   Exoplanet Classification Model Comparison\")\n",
        "\n",
        "# Tabs for different views\n",
        "tab1, tab2, tab3, tab4, tab5 = st.tabs([\n",
        "    \"üîç   PCA Analysis\",\n",
        "    \"üìä   Performance Overview\",\n",
        "    \"‚è±Ô∏è   Computational Efficiency\",\n",
        "    \"üéØ   Class-Specific Performance\",\n",
        "    \"üìâ   Confusion Matrices\"\n",
        "])\n",
        "\n",
        "# Tab 1: PCA Analysis\n",
        "with tab1:\n",
        "    st.header(\"Principal Component Analysis (PCA)\")\n",
        "\n",
        "    st.markdown(\"\"\"\n",
        "    This section shows the distribution of Kepler Objects of Interest (KOIs) in the Principal Component space.\n",
        "    PCA transforms the original high-dimensional feature space into a lower-dimensional representation that\n",
        "    captures the most variance in the data.\n",
        "    \"\"\")\n",
        "\n",
        "    col1, col2 = st.columns([3, 1])\n",
        "\n",
        "    with col1:\n",
        "        # Create 3D PCA plot\n",
        "        fig_3d = px.scatter_3d(\n",
        "            pca_df, x='PC1', y='PC2', z='PC3',\n",
        "            color='Class',\n",
        "            color_discrete_map=CLASS_COLORS,\n",
        "            title='3D PCA Visualization',\n",
        "            labels={\n",
        "                'PC1': f'PC1 ({explained_variance[0]:.0%})',\n",
        "                'PC2': f'PC2 ({explained_variance[1]:.0%})',\n",
        "                'PC3': f'PC3 ({explained_variance[2]:.0%})'\n",
        "            },\n",
        "            opacity=0.7,\n",
        "            height=700\n",
        "        )\n",
        "\n",
        "        # Update layout\n",
        "        fig_3d.update_layout(\n",
        "            scene=dict(\n",
        "                xaxis_title=f\"PC1 ({explained_variance[0]:.0%})\",\n",
        "                yaxis_title=f\"PC2 ({explained_variance[1]:.0%})\",\n",
        "                zaxis_title=f\"PC3 ({explained_variance[2]:.0%})\"\n",
        "            ),\n",
        "            legend=dict(\n",
        "                title=\"Exoplanet Class\",\n",
        "                font=dict(size=14)\n",
        "            ),\n",
        "            margin=dict(l=0, r=0, b=0, t=50)\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig_3d, use_container_width=True)\n",
        "\n",
        "    with col2:\n",
        "        st.subheader(\"Explained Variance\")\n",
        "\n",
        "        # Create a bar chart for explained variance\n",
        "        total_variance = sum(explained_variance[:3])\n",
        "\n",
        "        fig_var = px.bar(\n",
        "            x=['PC1', 'PC2', 'PC3'],\n",
        "            y=explained_variance,\n",
        "            title=f'Explained Variance Ratio (Total: {total_variance:.0%})',\n",
        "            labels={'x': 'Principal Component', 'y': 'Explained Variance Ratio'},\n",
        "            color_discrete_sequence=[COLORBLIND_PALETTE['purple']]\n",
        "        )\n",
        "\n",
        "        fig_var.update_layout(\n",
        "            xaxis_title=\"Principal Component\",\n",
        "            yaxis_title=\"Explained Variance Ratio\",\n",
        "            yaxis_tickformat='.0%',\n",
        "            showlegend=False\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig_var, use_container_width=True)\n",
        "\n",
        "        # Add text explanation\n",
        "        st.markdown(\"\"\"\n",
        "        The first 3 principal components capture approximately 85% of the variance in the dataset:\n",
        "\n",
        "        - **PC1** captures features related to transit depth and stellar parameters\n",
        "        - **PC2** relates to orbital characteristics\n",
        "        - **PC3** correlates with signal quality measurements\n",
        "\n",
        "        PCA helps visualize the natural clustering of exoplanet classes and reduces computational requirements for models.\n",
        "        \"\"\")\n",
        "\n",
        "    # Add 2D PCA Plot\n",
        "    st.subheader(\"2D PCA Visualization\")\n",
        "\n",
        "    # Create tabs for different 2D views\n",
        "    pca_2d_tabs = st.tabs([\"PC1 vs PC2\", \"PC2 vs PC3\", \"PC1 vs PC3\"])\n",
        "\n",
        "    with pca_2d_tabs[0]:\n",
        "        fig_2d_1_2 = px.scatter(\n",
        "            pca_df, x='PC1', y='PC2',\n",
        "            color='Class',\n",
        "            color_discrete_map=CLASS_COLORS,\n",
        "            title='PC1 vs PC2',\n",
        "            labels={\n",
        "                'PC1': f'PC1 ({explained_variance[0]:.0%})',\n",
        "                'PC2': f'PC2 ({explained_variance[1]:.0%})'\n",
        "            },\n",
        "            opacity=0.7,\n",
        "            height=500\n",
        "        )\n",
        "\n",
        "        fig_2d_1_2.update_layout(\n",
        "            xaxis_title=f\"PC1 ({explained_variance[0]:.0%})\",\n",
        "            yaxis_title=f\"PC2 ({explained_variance[1]:.0%})\",\n",
        "            legend=dict(\n",
        "                title=\"Exoplanet Class\",\n",
        "                font=dict(size=14)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig_2d_1_2, use_container_width=True)\n",
        "\n",
        "    with pca_2d_tabs[1]:\n",
        "        fig_2d_2_3 = px.scatter(\n",
        "            pca_df, x='PC2', y='PC3',\n",
        "            color='Class',\n",
        "            color_discrete_map=CLASS_COLORS,\n",
        "            title='PC2 vs PC3',\n",
        "            labels={\n",
        "                'PC2': f'PC2 ({explained_variance[1]:.0%})',\n",
        "                'PC3': f'PC3 ({explained_variance[2]:.0%})'\n",
        "            },\n",
        "            opacity=0.7,\n",
        "            height=500\n",
        "        )\n",
        "\n",
        "        fig_2d_2_3.update_layout(\n",
        "            xaxis_title=f\"PC2 ({explained_variance[1]:.0%})\",\n",
        "            yaxis_title=f\"PC3 ({explained_variance[2]:.0%})\",\n",
        "            legend=dict(\n",
        "                title=\"Exoplanet Class\",\n",
        "                font=dict(size=14)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig_2d_2_3, use_container_width=True)\n",
        "\n",
        "    with pca_2d_tabs[2]:\n",
        "        fig_2d_1_3 = px.scatter(\n",
        "            pca_df, x='PC1', y='PC3',\n",
        "            color='Class',\n",
        "            color_discrete_map=CLASS_COLORS,\n",
        "            title='PC1 vs PC3',\n",
        "            labels={\n",
        "                'PC1': f'PC1 ({explained_variance[0]:.0%})',\n",
        "                'PC3': f'PC3 ({explained_variance[2]:.0%})'\n",
        "            },\n",
        "            opacity=0.7,\n",
        "            height=500\n",
        "        )\n",
        "\n",
        "        fig_2d_1_3.update_layout(\n",
        "            xaxis_title=f\"PC1 ({explained_variance[0]:.0%})\",\n",
        "            yaxis_title=f\"PC3 ({explained_variance[2]:.0%})\",\n",
        "            legend=dict(\n",
        "                title=\"Exoplanet Class\",\n",
        "                font=dict(size=14)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig_2d_1_3, use_container_width=True)\n",
        "\n",
        "    # Add explanation\n",
        "    st.markdown(\"\"\"\n",
        "    ### PCA Analysis Insights:\n",
        "\n",
        "    - **Class Separation**: We can observe some natural separation between classes in the PCA space, especially between False Positives and Confirmed exoplanets\n",
        "\n",
        "    - **Candidate Overlap**: The Candidate class (blue) overlaps with both False Positives and Confirmed exoplanets, reflecting their uncertain status\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "# Tab 2: Performance Overview\n",
        "with tab2:\n",
        "    st.header(\"Model Performance Comparison\")\n",
        "\n",
        "    # Add information about the visualization\n",
        "    st.markdown(\"\"\"\n",
        "    This dashboard visualizes the performance of different machine learning models on the exoplanet classification task.\n",
        "    The models are evaluated on accuracy, precision, recall, and F1 score metrics.\n",
        "\n",
        "    Use the **sidebar** to filter models and PCA options. You can:\n",
        "    - Select specific models to compare\n",
        "    - Choose to view performances with or without PCA dimensionality reduction\n",
        "    \"\"\")\n",
        "\n",
        "    col1, col2 = st.columns([3, 1])\n",
        "\n",
        "    with col1:\n",
        "        # Performance metrics bar chart\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "        # Reshape data for plotting\n",
        "        plot_data = []\n",
        "        for _, row in filtered_metrics.iterrows():\n",
        "            for metric in metrics:\n",
        "                plot_data.append({\n",
        "                    'Model': f\"{row['model']} ({row['pca']})\",\n",
        "                    'Metric': metric.capitalize(),\n",
        "                    'Value': row[metric],\n",
        "                    'model': row['model'],\n",
        "                    'pca': row['pca']\n",
        "                })\n",
        "\n",
        "        plot_df = pd.DataFrame(plot_data)\n",
        "\n",
        "        # Create performance comparison chart\n",
        "        fig = px.bar(\n",
        "            plot_df,\n",
        "            x='Model',\n",
        "            y='Value',\n",
        "            color='Metric',\n",
        "            barmode='group',\n",
        "            color_discrete_sequence=[\n",
        "                COLORBLIND_PALETTE['blue'],\n",
        "                COLORBLIND_PALETTE['green'],\n",
        "                COLORBLIND_PALETTE['orange'],\n",
        "                COLORBLIND_PALETTE['purple']\n",
        "            ],\n",
        "            labels={'Value': 'Score', 'Model': ''},\n",
        "            hover_data=['model', 'pca', 'Value'],\n",
        "            title='Performance Metrics by Model',\n",
        "            text=plot_df['Value'].round(2)  # Add text showing the values\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            plot_bgcolor='rgba(0,0,0,0)',\n",
        "            height=500,\n",
        "            hovermode='closest',\n",
        "            legend=dict(\n",
        "                orientation='h',\n",
        "                yanchor='top',\n",
        "                y=1.15,\n",
        "                xanchor='right',\n",
        "                x=1\n",
        "            ),\n",
        "            margin=dict(b=150)  # Increased bottom margin for x-axis labels\n",
        "        )\n",
        "\n",
        "        # Add range slider for zooming\n",
        "        fig.update_layout(\n",
        "            xaxis=dict(\n",
        "                rangeslider=dict(visible=True),\n",
        "                type='category',\n",
        "                tickangle=45  # Angled labels to prevent overlap\n",
        "            ),\n",
        "            yaxis=dict(\n",
        "                range=[0.5, 1.0],  # Set a fixed range for the y-axis\n",
        "                title='Score'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Improve hover information\n",
        "        fig.update_traces(\n",
        "            hovertemplate='<b>%{customdata[0]} (%{customdata[1]})</b><br>%{x}<br>%{y:.4f}<extra>%{fullData.name}</extra>',\n",
        "            textposition='outside'  # Place text values outside the bars\n",
        "        )\n",
        "\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "        # Add best model annotation\n",
        "        best_model_info = filtered_metrics.sort_values('accuracy', ascending=False).iloc[0]\n",
        "        st.info(\n",
        "            f\"üí´   **Best Overall Model**: {best_model_info['model']} ({best_model_info['pca']}) with \"\n",
        "            f\"accuracy of {best_model_info['accuracy']:.4f} and F1 score of {best_model_info['f1']:.4f}\"\n",
        "        )\n",
        "\n",
        "    with col2:\n",
        "        # Top model per metric\n",
        "        st.subheader(\"Top Performer by Metric\")\n",
        "\n",
        "        for metric in metrics:\n",
        "            top_model = filtered_metrics.loc[filtered_metrics[metric].idxmax()]\n",
        "\n",
        "            # Create a styled metric display\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-box\">\n",
        "                <div class=\"metric-value\">{top_model[metric]:.4f}</div>\n",
        "                <div class=\"metric-label\">{metric.capitalize()}</div>\n",
        "                <div>{top_model['model']} ({top_model['pca']})</div>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # Feature importance or model explanation - For this demo, we'll use a static bar chart\n",
        "    # In a real implementation, this would come from your model data\n",
        "    st.subheader(\"Feature Importance\")\n",
        "\n",
        "    # Add explanation about the Importance Score\n",
        "    st.markdown(\"\"\"\n",
        "    The **Importance Score** represents the relative influence of each feature on the model's predictions.\n",
        "    These scores are calculated using **Permutation Importance**, which measures how much the model's performance\n",
        "    decreases when a feature's values are randomly shuffled. Higher scores indicate features that have a greater\n",
        "    impact on the model's decision-making process.\n",
        "\n",
        "    The importance scores shown here are averaged across all models (kNN, SVM, and Neural Network) to provide\n",
        "    a comprehensive view of which features are most influential in exoplanet classification.\n",
        "    \"\"\")\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': ['koi_fpflag_nt', 'koi_fpflag_ss', 'koi_period', 'koi_time0bk', 'koi_impact',\n",
        "                   'koi_duration', 'koi_depth', 'koi_prad', 'koi_teq', 'koi_insol'],\n",
        "        'Importance': [0.15, 0.12, 0.09, 0.08, 0.07, 0.11, 0.13, 0.09, 0.08, 0.08]\n",
        "    })\n",
        "\n",
        "    # Create a description dictionary for each feature's meaning\n",
        "    feature_descriptions = {\n",
        "        'koi_fpflag_nt': 'Not Transit-Like Flag',\n",
        "        'koi_fpflag_ss': 'Stellar Eclipse Flag',\n",
        "        'koi_period': 'Orbital Period (days)',\n",
        "        'koi_time0bk': 'Transit Epoch (BJD-2454833)',\n",
        "        'koi_impact': 'Impact Parameter',\n",
        "        'koi_duration': 'Transit Duration (hours)',\n",
        "        'koi_depth': 'Transit Depth (ppm)',\n",
        "        'koi_prad': 'Planet Radius (Earth radii)',\n",
        "        'koi_teq': 'Equilibrium Temperature (K)',\n",
        "        'koi_insol': 'Insolation Flux (Earth flux)'\n",
        "    }\n",
        "\n",
        "    # Add descriptions to the feature_importance DataFrame\n",
        "    feature_importance['Description'] = feature_importance['Feature'].map(feature_descriptions)\n",
        "\n",
        "    feature_fig = px.bar(\n",
        "        feature_importance.sort_values('Importance', ascending=False),\n",
        "        x='Importance',\n",
        "        y='Feature',\n",
        "        orientation='h',\n",
        "        color='Importance',\n",
        "        color_continuous_scale='Viridis',\n",
        "        title='Top 10 Most Important Features',\n",
        "        hover_data=['Description'],  # Add descriptions to hover information\n",
        "        text=feature_importance['Importance'].round(2)  # Add text labels\n",
        "    )\n",
        "\n",
        "    feature_fig.update_layout(\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        height=500,\n",
        "        yaxis={'categoryorder': 'total ascending'}\n",
        "    )\n",
        "\n",
        "    # Improve hover information\n",
        "    feature_fig.update_traces(\n",
        "        hovertemplate='<b>%{y}</b><br>Description: %{customdata[0]}<br>Importance: %{x:.2f}<extra></extra>',\n",
        "        textposition='outside'  # Place text values outside the bars\n",
        "    )\n",
        "\n",
        "    st.plotly_chart(feature_fig, use_container_width=True)\n",
        "\n",
        "    # Add detailed feature explanations\n",
        "    st.markdown(\"\"\"\n",
        "    ### Feature Descriptions\n",
        "\n",
        "    - **koi_fpflag_nt** (Not Transit-Like Flag): Indicates whether the signal resembles a non-planetary transit\n",
        "    - **koi_fpflag_ss** (Stellar Eclipse Flag): Indicates if the signal resembles a stellar eclipse\n",
        "    - **koi_depth** (Transit Depth): The fractional decrease in stellar brightness during a transit\n",
        "    - **koi_duration** (Transit Duration): Time duration of the planetary transit\n",
        "    - **koi_period** (Orbital Period): Time taken for the planet to orbit its star\n",
        "    - **koi_prad** (Planet Radius): Estimated radius of the planet in Earth radii\n",
        "    - **koi_teq** (Equilibrium Temperature): Estimated temperature of the planet\n",
        "    - **koi_impact** (Impact Parameter): Projected distance between planet and star centers during transit\n",
        "\n",
        "    These features provide critical information about the transit signal characteristics, orbital parameters, and\n",
        "    physical properties of the potential exoplanet, all of which help distinguish genuine planets from false positives.\n",
        "    \"\"\")\n",
        "\n",
        "    with col2:\n",
        "        # Top model per metric\n",
        "        st.subheader(\"Top Performer by Metric\")\n",
        "\n",
        "        for metric in metrics:\n",
        "            top_model = filtered_metrics.loc[filtered_metrics[metric].idxmax()]\n",
        "\n",
        "            # Create a styled metric display\n",
        "            st.markdown(f\"\"\"\n",
        "            <div class=\"metric-box\">\n",
        "                <div class=\"metric-value\">{top_model[metric]:.4f}</div>\n",
        "                <div class=\"metric-label\">{metric.capitalize()}</div>\n",
        "                <div>{top_model['model']} ({top_model['pca']})</div>\n",
        "            </div>\n",
        "            \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # Feature importance or model explanation - For this demo, we'll use a static bar chart\n",
        "    # In a real implementation, this would come from your model data\n",
        "    st.subheader(\"Feature Importance\")\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': ['koi_fpflag_nt', 'koi_fpflag_ss', 'koi_period', 'koi_time0bk', 'koi_impact',\n",
        "                   'koi_duration', 'koi_depth', 'koi_prad', 'koi_teq', 'koi_insol'],\n",
        "        'Importance': [0.15, 0.12, 0.09, 0.08, 0.07, 0.11, 0.13, 0.09, 0.08, 0.08]\n",
        "    })\n",
        "\n",
        "    feature_fig = px.bar(\n",
        "        feature_importance.sort_values('Importance', ascending=False),\n",
        "        x='Importance',\n",
        "        y='Feature',\n",
        "        orientation='h',\n",
        "        color='Importance',\n",
        "        color_continuous_scale='Viridis',\n",
        "        title='Top 10 Most Important Features'\n",
        "    )\n",
        "\n",
        "    feature_fig.update_layout(\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        height=400,\n",
        "        yaxis={'categoryorder': 'total ascending'},\n",
        "        annotations=[\n",
        "            dict(\n",
        "                x=0.5,\n",
        "                y=-0.15,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text='Feature importance values are averaged across all models',\n",
        "                showarrow=False,\n",
        "                font=dict(size=10)\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    st.plotly_chart(feature_fig, use_container_width=True)\n",
        "\n",
        "# Tab 3: Computational Efficiency\n",
        "with tab3:\n",
        "    st.header(\"Computational Efficiency\")\n",
        "    st.markdown(\"\"\"\n",
        "    This section shows the Neural Network model training progression from the DeepLearning notebook.\n",
        "    The plot displays the training and validation loss over 30 epochs, enabling analysis of the model's\n",
        "    learning curve and potential overfitting.\n",
        "    \"\"\")\n",
        "\n",
        "    # Neural Network Training & Validation Losses\n",
        "    st.subheader(\"Neural Network Training Progress\")\n",
        "\n",
        "    # Create interactive training/validation loss plot for Neural Network\n",
        "    nn_fig = go.Figure()\n",
        "\n",
        "    # Add traces for training and validation loss\n",
        "    nn_fig.add_trace(go.Scatter(\n",
        "        x=nn_history['epochs'],\n",
        "        y=nn_history['train_losses'],\n",
        "        mode='lines+markers',\n",
        "        name='Training Loss',\n",
        "        line=dict(color=MODEL_COLORS['Neural Network']),\n",
        "        hovertemplate='Epoch %{x}<br>Loss: %{y:.4f}'\n",
        "    ))\n",
        "\n",
        "    nn_fig.add_trace(go.Scatter(\n",
        "        x=nn_history['epochs'],\n",
        "        y=nn_history['val_losses'],\n",
        "        mode='lines+markers',\n",
        "        name='Validation Loss',\n",
        "        line=dict(color=COLORBLIND_PALETTE['cyan']),\n",
        "        hovertemplate='Epoch %{x}<br>Loss: %{y:.4f}'\n",
        "    ))\n",
        "\n",
        "    # Add annotations for final metrics\n",
        "    nn_fig.add_annotation(\n",
        "        x=28,\n",
        "        y=0.5,\n",
        "        text=f\"Final Test Accuracy: {nn_history['final_test_acc']:.2%}\",\n",
        "        showarrow=True,\n",
        "        arrowhead=1,\n",
        "        bgcolor=COLORBLIND_PALETTE['yellow'],\n",
        "        opacity=0.8\n",
        "    )\n",
        "\n",
        "    # Add vertical line at epoch 10 to indicate potential early stopping point\n",
        "    nn_fig.add_shape(\n",
        "        type=\"line\",\n",
        "        x0=10, y0=0,\n",
        "        x1=10, y1=2,\n",
        "        line=dict(color=\"red\", width=2, dash=\"dash\"),\n",
        "    )\n",
        "\n",
        "    nn_fig.add_annotation(\n",
        "        x=10,\n",
        "        y=2.5,\n",
        "        text=\"Potential Early Stopping Point\",\n",
        "        showarrow=True,\n",
        "        arrowhead=1,\n",
        "        ax=0,\n",
        "        ay=-40\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    nn_fig.update_layout(\n",
        "        title='Neural Network Training & Validation Loss',\n",
        "        xaxis_title='Epoch',\n",
        "        yaxis_title='Loss (log scale)',\n",
        "        yaxis_type='log',\n",
        "        hovermode='x unified',\n",
        "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        height=600\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    st.plotly_chart(nn_fig, use_container_width=True)\n",
        "\n",
        "    # Add explanatory text about the training curve\n",
        "    st.markdown(\"\"\"\n",
        "    ### Observations:\n",
        "\n",
        "    - **Initial High Loss**: The training loss starts very high (7008.92) and drops dramatically in the first few epochs\n",
        "\n",
        "    - **Convergence**: Around epoch 10, both training and validation losses stabilize, suggesting that training beyond this point provides diminishing returns\n",
        "\n",
        "    - **Gap Between Curves**: After epoch 10, the training loss continues to decrease slightly while validation loss remains flat or increases slightly, indicating potential overfitting\n",
        "\n",
        "    - **Final Performance**: The model achieves a final test accuracy of 50%, which indicates the Neural Network is struggling with generalization to unseen data\n",
        "\n",
        "    ### Insight:\n",
        "\n",
        "    - Consider implementing early stopping at around epoch 10\n",
        "    - Use PCA for dimensionality reduction to improve generalization\n",
        "    \"\"\")\n",
        "\n",
        "\n",
        "# Tab 4: Class-Specific Performance\n",
        "with tab4:\n",
        "    st.header(\"Class-Specific Performance Comparison\")\n",
        "    st.markdown(\"\"\"\n",
        "    This section breaks down model performance for each exoplanet class.\n",
        "    Analyze how each model performs on different classes and understand where they excel or struggle.\n",
        "    \"\"\")\n",
        "\n",
        "    # Class selector\n",
        "    selected_class = st.selectbox(\n",
        "        \"Select Class to Analyze\",\n",
        "        options=['All Classes', 'FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED']\n",
        "    )\n",
        "\n",
        "    if selected_class == 'All Classes':\n",
        "        class_filter = ['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED']\n",
        "    else:\n",
        "        class_filter = [selected_class]\n",
        "\n",
        "    class_data = filtered_class_metrics[filtered_class_metrics['class'].isin(class_filter)]\n",
        "\n",
        "    # Metric selector\n",
        "    selected_metric = st.radio(\n",
        "        \"Select Performance Metric\",\n",
        "        options=['F1 Score', 'Precision', 'Recall'],\n",
        "        horizontal=True\n",
        "    )\n",
        "\n",
        "    metric_map = {'F1 Score': 'f1', 'Precision': 'precision', 'Recall': 'recall'}\n",
        "    selected_metric_key = metric_map[selected_metric]\n",
        "\n",
        "    # Create class-specific performance bar chart\n",
        "    class_fig = px.bar(\n",
        "        class_data,\n",
        "        x='model',\n",
        "        y=selected_metric_key,\n",
        "        color='class',\n",
        "        facet_col='pca' if len(pca_filter) > 1 else None,\n",
        "        barmode='group',\n",
        "        color_discrete_map=CLASS_COLORS,\n",
        "        labels={\n",
        "            selected_metric_key: selected_metric,\n",
        "            'model': 'Model',\n",
        "            'class': 'Exoplanet Class',\n",
        "            'pca': 'PCA Option'\n",
        "        },\n",
        "        title=f'{selected_metric} by Class and Model',\n",
        "        height=500\n",
        "    )\n",
        "\n",
        "    class_fig.update_layout(\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        hovermode='closest',\n",
        "        legend=dict(\n",
        "            orientation='h',\n",
        "            yanchor='bottom',\n",
        "            y=1.02,\n",
        "            xanchor='right',\n",
        "            x=1\n",
        "        ),\n",
        "        annotations=[\n",
        "            dict(\n",
        "                x=0.5,\n",
        "                y=-0.15,\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text='Higher values indicate better performance on that class',\n",
        "                showarrow=False,\n",
        "                font=dict(size=12)\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Improve hover information\n",
        "    class_fig.update_traces(\n",
        "        hovertemplate='<b>%{x}</b><br>Class: %{data.name}<br>%{y:.4f}<extra></extra>'\n",
        "    )\n",
        "\n",
        "    # Fixed y-axis range for better comparison\n",
        "    class_fig.update_layout(yaxis_range=[0.0, 1.0])\n",
        "\n",
        "    st.plotly_chart(class_fig, use_container_width=True)\n",
        "\n",
        "    # Add information about class distribution\n",
        "    st.subheader(\"Class Distribution in Dataset\")\n",
        "    class_dist = pd.DataFrame({\n",
        "        'Class': ['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED'],\n",
        "        'Count': [3744, 1425, 2616],\n",
        "        'Percentage': [48.09, 18.30, 33.60]\n",
        "    })\n",
        "    # Create a pie chart\n",
        "    dist_fig = px.pie(\n",
        "        class_dist,\n",
        "        values='Count',\n",
        "        names='Class',\n",
        "        color='Class',\n",
        "        color_discrete_map=CLASS_COLORS,\n",
        "        title='Class Distribution in Dataset',\n",
        "        hover_data=['Percentage'],\n",
        "        labels={'Percentage': 'Percentage (%)'}\n",
        "    )\n",
        "    dist_fig.update_traces(\n",
        "        textposition='inside',\n",
        "        textinfo='percent+label',\n",
        "        hovertemplate='<b>%{label}</b><br>Count: %{value}<br>Percentage: %{customdata[0]:.2f}%<extra></extra>'\n",
        "    )\n",
        "    dist_fig.update_layout(\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        height=400,\n",
        "        legend=dict(\n",
        "            orientation='h',\n",
        "            yanchor='bottom',\n",
        "            y=-0.1,\n",
        "            xanchor='center',\n",
        "            x=0.5\n",
        "        )\n",
        "    )\n",
        "    col1, col2 = st.columns([3, 2])\n",
        "    with col1:\n",
        "        st.plotly_chart(dist_fig, use_container_width=True)\n",
        "    with col2:\n",
        "        st.markdown(\"\"\"\n",
        "        ### Class Imbalance Impact\n",
        "        The dataset has significant class imbalance:\n",
        "        - **FALSE POSITIVE**: 48.09% (majority class)\n",
        "        - **CONFIRMED**: 33.60%\n",
        "        - **CANDIDATE**: 18.30% (minority class)\n",
        "\n",
        "        This imbalance may explain why models typically perform better on FALSE POSITIVE and CONFIRMED classes,\n",
        "        and struggle more with the CANDIDATE class.\n",
        "        When evaluating models, consider using F1 score as it balances precision and recall,\n",
        "        which is important in imbalanced datasets.\n",
        "        \"\"\")\n",
        "    # Per-class best model analysis\n",
        "    st.subheader(\"Best Model per Class\")\n",
        "    # Group by class and find best model for each class based on selected metric\n",
        "    best_by_class = class_data.loc[class_data.groupby('class')[selected_metric_key].idxmax()]\n",
        "    # Create a comparison table\n",
        "    best_table = best_by_class[['class', 'model', 'pca', selected_metric_key]]\n",
        "    best_table.columns = ['Class', 'Best Model', 'PCA Option', selected_metric]\n",
        "    # Convert to styled DataFrame for display\n",
        "    styled_table = best_table.style.background_gradient(subset=[selected_metric], cmap='Blues')\n",
        "    st.dataframe(styled_table, use_container_width=True)\n",
        "    # Class performance heatmap\n",
        "    st.subheader(\"Class Performance Heatmap\")\n",
        "    st.markdown(\"\"\"\n",
        "    This heatmap shows how each model performs across different classes,\n",
        "    highlighting strengths and weaknesses for each class-model combination.\n",
        "    \"\"\")\n",
        "    # Create a pivot table for the heatmap\n",
        "    if len(pca_filter) == 1:\n",
        "        # Single PCA option selected\n",
        "        pivot_data = class_data.pivot_table(\n",
        "            values=selected_metric_key,\n",
        "            index='class',\n",
        "            columns='model'\n",
        "        )\n",
        "        # Create heatmap\n",
        "        heatmap_fig = px.imshow(\n",
        "            pivot_data,\n",
        "            color_continuous_scale='Viridis',\n",
        "            labels=dict(x='Model', y='Class', color=selected_metric),\n",
        "            title=f'{selected_metric} Heatmap by Class and Model ({pca_filter[0]})',\n",
        "            text_auto='.4f',\n",
        "            aspect='auto',\n",
        "            height=400\n",
        "        )\n",
        "    else:\n",
        "        # Both PCA options selected - create a faceted heatmap\n",
        "        facet_col = st.radio(\"Group heatmap by:\", options=['Model', 'PCA Option'], horizontal=True)\n",
        "        if facet_col == 'Model':\n",
        "            # Create multiple heatmaps - one for each model\n",
        "            heatmap_fig = make_subplots(\n",
        "                rows=1,\n",
        "                cols=len(selected_models),\n",
        "                subplot_titles=[f\"{model}\" for model in selected_models]\n",
        "            )\n",
        "            for i, model in enumerate(selected_models):\n",
        "                model_data = class_data[class_data['model'] == model].pivot_table(\n",
        "                    values=selected_metric_key,\n",
        "                    index='class',\n",
        "                    columns='pca'\n",
        "                )\n",
        "                heatmap_fig.add_trace(\n",
        "                    go.Heatmap(\n",
        "                        z=model_data.values,\n",
        "                        x=model_data.columns,\n",
        "                        y=model_data.index,\n",
        "                        colorscale='Viridis',\n",
        "                        showscale=i == 0,  # Only show colorbar for first heatmap\n",
        "                        text=model_data.values,\n",
        "                        texttemplate='%{text:.4f}',\n",
        "                        hovertemplate='<b>%{y}</b><br>PCA: %{x}<br>Value: %{z:.4f}<extra></extra>'\n",
        "                    ),\n",
        "                    row=1, col=i+1\n",
        "                )\n",
        "        else:  # Group by PCA\n",
        "            heatmap_fig = make_subplots(\n",
        "                rows=1,\n",
        "                cols=len(pca_filter),\n",
        "                subplot_titles=[f\"{pca}\" for pca in pca_filter]\n",
        "            )\n",
        "            for i, pca in enumerate(pca_filter):\n",
        "                pca_data = class_data[class_data['pca'] == pca].pivot_table(\n",
        "                    values=selected_metric_key,\n",
        "                    index='class',\n",
        "                    columns='model'\n",
        "                )\n",
        "                heatmap_fig.add_trace(\n",
        "                    go.Heatmap(\n",
        "                        z=pca_data.values,\n",
        "                        x=pca_data.columns,\n",
        "                        y=pca_data.index,\n",
        "                        colorscale='Viridis',\n",
        "                        showscale=i == 0,  # Only show colorbar for first heatmap\n",
        "                        text=pca_data.values,\n",
        "                        texttemplate='%{text:.4f}',\n",
        "                        hovertemplate='<b>%{y}</b><br>Model: %{x}<br>Value: %{z:.4f}<extra></extra>'\n",
        "                    ),\n",
        "                    row=1, col=i+1\n",
        "                )\n",
        "            heatmap_fig.update_layout(\n",
        "                height=400,\n",
        "                title_text=f'{selected_metric} Heatmap Comparison'\n",
        "            )\n",
        "    heatmap_fig.update_layout(plot_bgcolor='rgba(0,0,0,0)')\n",
        "    st.plotly_chart(heatmap_fig, use_container_width=True)\n",
        "\n",
        "# Tab 5: Confusion Matrices\n",
        "with tab5:\n",
        "    st.header(\"Confusion Matrices Analysis\")\n",
        "    st.markdown(\"\"\"\n",
        "    Confusion matrices show how well each model classifies each class.\n",
        "    - The diagonal elements represent **correct** classifications.\n",
        "    - Off-diagonal elements represent **misclassifications**.\n",
        "    Analyze how different models misclassify each class and understand error patterns.\n",
        "    \"\"\")\n",
        "    # Model selector for confusion matrix\n",
        "    cm_model = st.selectbox(\n",
        "        \"Select Model\",\n",
        "        options=selected_models\n",
        "    )\n",
        "    # PCA option for confusion matrix\n",
        "    cm_pca = st.radio(\n",
        "        \"Select PCA Option\",\n",
        "        options=pca_filter,\n",
        "        horizontal=True\n",
        "    )\n",
        "    # Get the confusion matrix for the selected model and PCA option\n",
        "    confusion_matrix = confusion_matrices[cm_model][cm_pca]\n",
        "    # Create a DataFrame for the confusion matrix\n",
        "    cm_df = pd.DataFrame(\n",
        "        confusion_matrix,\n",
        "        index=['TRUE: FALSE POSITIVE', 'TRUE: CANDIDATE', 'TRUE: CONFIRMED'],\n",
        "        columns=['PRED: FALSE POSITIVE', 'PRED: CANDIDATE', 'PRED: CONFIRMED']\n",
        "    )\n",
        "    # Create a heatmap for the confusion matrix\n",
        "    cm_fig = px.imshow(\n",
        "        cm_df,\n",
        "        color_continuous_scale='Blues',\n",
        "        labels=dict(x='Predicted Class', y='True Class', color='Count'),\n",
        "        title=f'Confusion Matrix - {cm_model} ({cm_pca})',\n",
        "        text_auto=True,\n",
        "        aspect='auto',\n",
        "        height=600  # Increased height for better spacing\n",
        "    )\n",
        "    cm_fig.update_layout(\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        xaxis=dict(\n",
        "            side='top',\n",
        "            tickangle=0  # Horizontal labels\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            tickangle=0  # Horizontal labels\n",
        "        ),\n",
        "        margin=dict(l=150, r=100, t=150, b=100),  # Increased margins around the matrix\n",
        "        annotations=[\n",
        "            dict(\n",
        "                x=0.5,\n",
        "                y=-0.25,  # Moved annotation lower to avoid overlap\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text='Higher values on the diagonal indicate better performance',\n",
        "                showarrow=False,\n",
        "                font=dict(size=12)\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    # Improve hover information\n",
        "    cm_fig.update_traces(\n",
        "        hovertemplate='<b>%{y}</b><br>%{x}<br>Count: %{z}<extra></extra>'\n",
        "    )\n",
        "    col1, col2 = st.columns([3, 2])\n",
        "    with col1:\n",
        "        st.plotly_chart(cm_fig, use_container_width=True)\n",
        "    with col2:\n",
        "        # Calculate and display derived metrics\n",
        "        total = confusion_matrix.sum()\n",
        "        diag = np.diag(confusion_matrix).sum()\n",
        "        accuracy = diag / total\n",
        "        class_names = ['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED']\n",
        "        # Calculate per-class metrics\n",
        "        metrics_data = []\n",
        "        for i, class_name in enumerate(class_names):\n",
        "            true_pos = confusion_matrix[i, i]\n",
        "            false_pos = confusion_matrix[:, i].sum() - true_pos\n",
        "            false_neg = confusion_matrix[i, :].sum() - true_pos\n",
        "            precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
        "            recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "            metrics_data.append({\n",
        "                'Class': class_name,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1 Score': f1\n",
        "            })\n",
        "        metrics_df = pd.DataFrame(metrics_data)\n",
        "        # Display overall accuracy\n",
        "        st.metric(\n",
        "            label=\"Overall Accuracy\",\n",
        "            value=f\"{accuracy:.4f}\",\n",
        "            delta=f\"{accuracy - 0.7:.4f}\" if accuracy > 0.7 else None\n",
        "        )\n",
        "        # Display per-class metrics\n",
        "        st.markdown(\"### Per-Class Metrics\")\n",
        "        # Convert to styled DataFrame for display\n",
        "        styled_metrics = metrics_df.style.background_gradient(subset=['F1 Score'], cmap='Blues')\n",
        "        st.dataframe(styled_metrics, use_container_width=True)\n",
        "        # Add error analysis\n",
        "        st.markdown(\"### Error Analysis\")\n",
        "        # Find the most common misclassification\n",
        "        off_diag = confusion_matrix.copy()\n",
        "        np.fill_diagonal(off_diag, 0)\n",
        "        max_error_idx = np.unravel_index(off_diag.argmax(), off_diag.shape)\n",
        "        error_from = class_names[max_error_idx[0]]\n",
        "        error_to = class_names[max_error_idx[1]]\n",
        "        error_count = off_diag[max_error_idx]\n",
        "        st.markdown(f\"\"\"\n",
        "        **Most common error**: {error_count} instances of **{error_from}**\n",
        "        misclassified as **{error_to}**.\n",
        "        This suggests the model may have difficulty distinguishing between\n",
        "        these two classes. Consider:\n",
        "        - Feature engineering to better separate these classes\n",
        "        - Adding more training examples for these classes\n",
        "        - Using class weights to balance the training\n",
        "        \"\"\")\n",
        "    # Add a normalized confusion matrix\n",
        "    st.subheader(\"Normalized Confusion Matrix\")\n",
        "    st.markdown(\"\"\"\n",
        "    Normalized confusion matrices show the percentage of samples in each true class\n",
        "    that are classified as each predicted class. This helps to understand the performance\n",
        "    on each class regardless of its size in the dataset.\n",
        "    \"\"\")\n",
        "    # Normalize the confusion matrix (by row)\n",
        "    row_sums = confusion_matrix.sum(axis=1)\n",
        "    norm_cm = confusion_matrix / row_sums[:, np.newaxis]\n",
        "    # Create a DataFrame for the normalized confusion matrix\n",
        "    norm_cm_df = pd.DataFrame(\n",
        "        norm_cm,\n",
        "        index=['TRUE: FALSE POSITIVE', 'TRUE: CANDIDATE', 'TRUE: CONFIRMED'],\n",
        "        columns=['PRED: FALSE POSITIVE', 'PRED: CANDIDATE', 'PRED: CONFIRMED']\n",
        "    )\n",
        "    # Create a heatmap for the normalized confusion matrix\n",
        "    norm_cm_fig = px.imshow(\n",
        "        norm_cm_df,\n",
        "        color_continuous_scale='Blues',\n",
        "        labels=dict(x='Predicted Class', y='True Class', color='Proportion'),\n",
        "        title=f'Normalized Confusion Matrix - {cm_model} ({cm_pca})',\n",
        "        text_auto='.2%',\n",
        "        aspect='auto',\n",
        "        height=600  # Increased height for better spacing\n",
        "    )\n",
        "    norm_cm_fig.update_layout(\n",
        "        plot_bgcolor='rgba(0,0,0,0)',\n",
        "        xaxis=dict(\n",
        "            side='top',\n",
        "            tickangle=0  # Horizontal labels\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            tickangle=0  # Horizontal labels\n",
        "        ),\n",
        "        margin=dict(l=150, r=100, t=150, b=100),  # Increased margins around the matrix\n",
        "        annotations=[\n",
        "            dict(\n",
        "                x=0.5,\n",
        "                y=-0.25,  # Moved annotation lower to avoid overlap\n",
        "                xref='paper',\n",
        "                yref='paper',\n",
        "                text='Values represent the proportion of each true class classified as each predicted class',\n",
        "                showarrow=False,\n",
        "                font=dict(size=12)\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    # Improve hover information\n",
        "    norm_cm_fig.update_traces(\n",
        "        hovertemplate='<b>%{y}</b><br>%{x}<br>Proportion: %{z:.2%}<extra></extra>'\n",
        "    )\n",
        "    st.plotly_chart(norm_cm_fig, use_container_width=True)\n",
        "    # Confusion matrix comparison\n",
        "    if len(selected_models) > 1 and len(pca_filter) > 0:\n",
        "        st.subheader(\"Confusion Matrix Comparison\")\n",
        "        st.markdown(\"\"\"\n",
        "        Compare confusion matrices across different models and PCA options to identify\n",
        "        which model performs best for specific classification tasks.\n",
        "        \"\"\")\n",
        "        # Choose what to compare\n",
        "        if len(selected_models) > 1 and len(pca_filter) > 1:\n",
        "            compare_option = st.radio(\n",
        "                \"Compare across:\",\n",
        "                options=['Models (same PCA)', 'PCA Options (same model)'],\n",
        "                horizontal=True\n",
        "            )\n",
        "            if compare_option == 'Models (same PCA)':\n",
        "                # Compare different models with the same PCA option\n",
        "                compare_pca = st.selectbox(\n",
        "                    \"Select PCA Option for Comparison\",\n",
        "                    options=pca_filter\n",
        "                )\n",
        "                models_to_compare = selected_models\n",
        "                fixed_pca = compare_pca\n",
        "                # Create subplots for each model\n",
        "                cm_comparison_fig = make_subplots(\n",
        "                    rows=1,\n",
        "                    cols=len(models_to_compare),\n",
        "                    subplot_titles=[f\"{model} ({fixed_pca})\" for model in models_to_compare],\n",
        "                    horizontal_spacing=0.1  # Increased spacing between subplots\n",
        "                )\n",
        "                for i, model in enumerate(models_to_compare):\n",
        "                    cm = confusion_matrices[model][fixed_pca]\n",
        "                    cm_comparison_fig.add_trace(\n",
        "                        go.Heatmap(\n",
        "                            z=cm,\n",
        "                            x=['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED'],\n",
        "                            y=['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED'],\n",
        "                            colorscale='Blues',\n",
        "                            showscale=i == 0,  # Only show colorbar for first heatmap\n",
        "                            text=cm,\n",
        "                            texttemplate='%{text}',\n",
        "                            hovertemplate='<b>True: %{y}</b><br>Predicted: %{x}<br>Count: %{z}<extra></extra>'\n",
        "                        ),\n",
        "                        row=1, col=i+1\n",
        "                    )\n",
        "            else:\n",
        "                # Compare different PCA options for the same model\n",
        "                compare_model = st.selectbox(\n",
        "                    \"Select Model for Comparison\",\n",
        "                    options=selected_models\n",
        "                )\n",
        "                pcas_to_compare = pca_filter\n",
        "                fixed_model = compare_model\n",
        "                # Create subplots for each PCA option\n",
        "                cm_comparison_fig = make_subplots(\n",
        "                    rows=1,\n",
        "                    cols=len(pcas_to_compare),\n",
        "                    subplot_titles=[f\"{fixed_model} ({pca})\" for pca in pcas_to_compare],\n",
        "                    horizontal_spacing=0.1  # Increased spacing between subplots\n",
        "                )\n",
        "                for i, pca in enumerate(pcas_to_compare):\n",
        "                    cm = confusion_matrices[fixed_model][pca]\n",
        "                    cm_comparison_fig.add_trace(\n",
        "                        go.Heatmap(\n",
        "                            z=cm,\n",
        "                            x=['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED'],\n",
        "                            y=['FALSE POSITIVE', 'CANDIDATE', 'CONFIRMED'],\n",
        "                            colorscale='Blues',\n",
        "                            showscale=i == 0,  # Only show colorbar for first heatmap\n",
        "                            text=cm,\n",
        "                            texttemplate='%{text}',\n",
        "                            hovertemplate='<b>True: %{y}</b><br>Predicted: %{x}<br>Count: %{z}<extra></extra>'\n",
        "                        ),\n",
        "                        row=1, col=i+1\n",
        "                    )\n",
        "        else:\n",
        "            # Only one model or PCA option selected\n",
        "            st.info(\"Please select multiple models and PCA options in the sidebar to enable comparison.\")\n",
        "            cm_comparison_fig = None\n",
        "        if cm_comparison_fig:\n",
        "            cm_comparison_fig.update_layout(\n",
        "                height=500,\n",
        "                title_text='Confusion Matrix Comparison',\n",
        "                margin=dict(l=150, r=100, t=150, b=100)  # Increased margins\n",
        "            )\n",
        "            # Update all axes\n",
        "            for i in range(len(cm_comparison_fig.data)):\n",
        "                cm_comparison_fig.update_xaxes(\n",
        "                    title_text=\"Predicted Class\",\n",
        "                    row=1,\n",
        "                    col=i+1,\n",
        "                    tickangle=0  # Horizontal labels\n",
        "                )\n",
        "                cm_comparison_fig.update_yaxes(\n",
        "                    title_text=\"True Class\",\n",
        "                    row=1,\n",
        "                    col=i+1,\n",
        "                    tickangle=0  # Horizontal labels\n",
        "                )\n",
        "            st.plotly_chart(cm_comparison_fig, use_container_width=True)\n",
        "\n",
        "# Add a summary section at the bottom\n",
        "st.markdown(\"---\")\n",
        "st.header(\"Summary and Recommendations\")\n",
        "col1, col2 = st.columns(2)\n",
        "with col1:\n",
        "  st.subheader(\"Key Findings\")\n",
        "  # Find best model overall\n",
        "  best_model_info = filtered_metrics.sort_values('accuracy', ascending=False).iloc[0]\n",
        "  best_model = best_model_info['model']\n",
        "  best_pca = best_model_info['pca']\n",
        "  best_accuracy = best_model_info['accuracy']\n",
        "  # Find fastest model\n",
        "  fastest_model_info = filtered_metrics.sort_values('training_time').iloc[0]\n",
        "  fastest_model = fastest_model_info['model']\n",
        "  fastest_pca = fastest_model_info['pca']\n",
        "  fastest_time = fastest_model_info['training_time']\n",
        "  # Find most memory-efficient model\n",
        "  mem_eff_model_info = filtered_metrics.sort_values('memory_usage').iloc[0]\n",
        "  mem_eff_model = mem_eff_model_info['model']\n",
        "  mem_eff_pca = mem_eff_model_info['pca']\n",
        "  mem_eff_usage = mem_eff_model_info['memory_usage']\n",
        "  st.markdown(f\"\"\"\n",
        "  Based on the analysis, the key findings are:\n",
        "  1. **Best Overall Model**: {best_model} ({best_pca}) with accuracy of {best_accuracy:.4f}\n",
        "  2. **Fastest Model**: {fastest_model} ({fastest_pca}) with training time of {fastest_time:.2f} seconds\n",
        "  3. **Most Memory-Efficient**: {mem_eff_model} ({mem_eff_pca}) using {mem_eff_usage:.2f} MB\n",
        "  4. **Class Performance**:\n",
        "      - All models perform best on the FALSE POSITIVE class\n",
        "      - The CANDIDATE class is the most challenging to classify correctly\n",
        "  5. **PCA Impact**:\n",
        "      - PCA generally reduces training time and memory usage (except for SVM where it increased training time by 4.03%)\n",
        "      - PCA slightly reduces model performance in most cases\n",
        "  6. **Neural Network Performance**:\n",
        "      - The Neural Network model without PCA shows clear signs of overfitting, with training loss decreasing while validation loss plateaus\n",
        "      - With PCA, the Neural Network shows better generalization with closer training and validation losses\n",
        "  \"\"\")\n",
        "\n",
        "with col2:\n",
        "    st.subheader(\"Recommendations\")\n",
        "    # Find the model with the best balance of performance and efficiency\n",
        "    # Simple scoring approach: normalize metrics and combine\n",
        "    performance_metrics = filtered_metrics.copy()\n",
        "    # Normalize metrics (higher is better for accuracy, lower is better for time/memory)\n",
        "    max_acc = performance_metrics['accuracy'].max()\n",
        "    min_acc = performance_metrics['accuracy'].min()\n",
        "    max_time = performance_metrics['training_time'].max()\n",
        "    min_time = performance_metrics['training_time'].min()\n",
        "    max_mem = performance_metrics['memory_usage'].max()\n",
        "    min_mem = performance_metrics['memory_usage'].min()\n",
        "    if max_acc > min_acc:\n",
        "        performance_metrics['norm_acc'] = (performance_metrics['accuracy'] - min_acc) / (max_acc - min_acc)\n",
        "    else:\n",
        "        performance_metrics['norm_acc'] = 1.0\n",
        "    if max_time > min_time:\n",
        "        performance_metrics['norm_time'] = (max_time - performance_metrics['training_time']) / (max_time - min_time)\n",
        "    else:\n",
        "        performance_metrics['norm_time'] = 1.0\n",
        "    if max_mem > min_mem:\n",
        "        performance_metrics['norm_mem'] = (max_mem - performance_metrics['memory_usage']) / (max_mem - min_mem)\n",
        "    else:\n",
        "        performance_metrics['norm_mem'] = 1.0\n",
        "    # Combine into a single score (equal weighting)\n",
        "    performance_metrics['balance_score'] = (\n",
        "        performance_metrics['norm_acc'] * 0.5 +\n",
        "        performance_metrics['norm_time'] * 0.25 +\n",
        "        performance_metrics['norm_mem'] * 0.25\n",
        "    )\n",
        "    # Find the best balanced model\n",
        "    balanced_model_info = performance_metrics.sort_values('balance_score', ascending=False).iloc[0]\n",
        "    balanced_model = balanced_model_info['model']\n",
        "    balanced_pca = balanced_model_info['pca']\n",
        "    balanced_acc = balanced_model_info['accuracy']\n",
        "    balanced_time = balanced_model_info['training_time']\n",
        "    balanced_mem = balanced_model_info['memory_usage']\n",
        "    st.markdown(f\"\"\"\n",
        "    Based on the analysis, the recommendations are:\n",
        "    1. **For Maximum Accuracy**: Use {best_model} ({best_pca})\n",
        "       - Best for critical applications where performance is paramount\n",
        "       - Consider the additional computational cost\n",
        "    2. **For Fast Development**: Use {fastest_model} ({fastest_pca})\n",
        "       - Best for rapid prototyping and iteration\n",
        "       - Good choice for real-time applications\n",
        "    3. **For Resource-Constrained Environments**: Use {mem_eff_model} ({mem_eff_pca})\n",
        "       - Best for deployment on limited hardware\n",
        "       - Good for batch processing multiple datasets\n",
        "    4. **Best Balance (Recommended)**: Use {balanced_model} ({balanced_pca})\n",
        "       - Accuracy: {balanced_acc:.4f}\n",
        "       - Training Time: {balanced_time:.2f} seconds\n",
        "       - Memory Usage: {balanced_mem:.2f} MB\n",
        "       - Provides the best trade-off between performance and resource usage\n",
        "    5. **For the Neural Network Model**:\n",
        "       - Use PCA to reduce overfitting and improve generalization\n",
        "       - Consider early stopping around epoch 10 to prevent overfitting\n",
        "       - Implement regularization techniques for better performance on the CANDIDATE class\n",
        "    \"\"\")\n",
        "\"\"\")\n",
        "'''\n",
        "\n",
        "print(\"Dashboard code has been saved to 'kepler_dashboard.py'\")\n",
        "\n",
        "# Start Streamlit with the fixed file\n",
        "import subprocess\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit in the background\n",
        "print(\"Starting Streamlit with fixed file...\")\n",
        "process = subprocess.Popen(['streamlit', 'run', 'fixed_dashboard.py', '--server.port', '8501'],\n",
        "                          stdout=subprocess.PIPE,\n",
        "                          stderr=subprocess.PIPE)\n",
        "\n",
        "# Wait for Streamlit to start\n",
        "time.sleep(15)  # Give it time to fully start\n",
        "\n",
        "# Set authtoken here\n",
        "ngrok.set_auth_token(\"2vks47C4VQoVVGC66ci0cSdrFZj_3vTxD7BmVu9jK7bQkayWa\")\n",
        "\n",
        "# Set up a tunnel to the Streamlit app\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app URL: {public_url}\")\n",
        "print(\"Click the URL above to view dashboard\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
